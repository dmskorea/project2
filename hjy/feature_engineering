# ---
# 2019-08-19
# ---

TOP VI : TransactionAmt, card1 !!!!

log(x), log(x+1), sqrt(x), x^2
Signif
Round
NumToCat
Interaction Features
Indicator Features
PCA
svd
Cluster
Weight of Evidence 
CV Target Mean Encoding 
knn features
NULL featurs

train_clean['TransactionAmt'] = np.log(train_clean['TransactionAmt']+1)
train_clean['dist1'] = np.log(train_clean['dist1']+1)
train_clean['dist2'] = np.log(train_clean['dist2']+1)

test_clean['TransactionAmt'] = np.log(test_clean['TransactionAmt']+1)
test_clean['dist1'] = np.log(test_clean['dist1']+1)
test_clean['dist2'] = np.log(test_clean['dist2']+1)

# ---
# 2019-08-18
# ---

ProductCD : C
ProductCD * TransactionAmt
Card3 : 185, 119, 144
Card5 : 137
Card4 : discover
Card4_discover의 TransactionAmt
M4 : M2
addr1_251의 TransactionAmt
addr2 : 65
P_emaildomain : mail.com
R_emaildomain : icloud.com, Google
C1 : 0
C1 19의 TransactionAmt
C2는 numeric으로 변환
Days  : 29, 30, 31
Hours : 5~12
id_12 : NaN
id_15 : NaN
id_16 : NaN
id_23 : PROXY_ANONY.., NaN
id_28 : NaN
id_29 : NaN
id_30 : others, func
id_31 : others

# ---
# 2019-08-16
# ---

https://www.kaggle.com/jesucristo/fraud-complete-eda#Submission

[indicator features]

R_emaildomain : Google, icloud.com
Day : 1, 29, 30, 31
Hours : 4 - 12
Hours : 5 - 11
Hours : 6 - 10

C2는 numeric으로 변수 만들기 

# ---
# 2019-08-15
# ---

https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt

We can see that Card 1 and Card 2 has a large distribution of values, so maybe it will be better to get the log of these columns

df_trans.loc[df_trans.card3.isin(df_trans.card3.value_counts()[df_trans.card3.value_counts() < 200].index), 'card3'] = "Others"
df_trans.loc[df_trans.card5.isin(df_trans.card5.value_counts()[df_trans.card5.value_counts() < 300].index), 'card5'] = "Others"

In Card3 we can see that 100 and 106 are the most common values in the column. 
We have 4.95% of Frauds in 100 and 1.52% in 106; The values with highest Fraud Transactions are 185, 119 and 119; 

In card5 the most frequent values are 226, 224, 166 that represents 73% of data. Also is posible to see high % of frauds in 137, 147, 141 that has few entries for values.

df_train['TransactionAmt'] = np.log(df_train['TransactionAmt'])
df_test['TransactionAmt'] = np.log(df_test['TransactionAmt'])

https://www.kaggle.com/iasnobmatsu/xgb-model-with-feature-engineering

# ---
# 2019-08-10
# ---

# TransactionDT 날짜 변환 (86400 -> 2017-12-02 00:00:00)
train['TransactionDT'] = train['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))

# sum of missing values
train['nulls1'] = train.isna().sum(axis=1)

# 최신 브라우저 여부
# train = setbrowser(train)
train = setbrowser(train)

# email 주소 파싱(_bin/_suffix)
_bin : 이메일 카테고리 그룹핑
_suffix : 이메일주소 마지막 단어 (com, net, etc)

# 거래금액 소수점
train['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)

# encoding 

# card1~6 (카드정보 count encoding)
# card4: 카드종류1, visa/mastercard etc..
# card6: 카드종류2, debit/credet etc..
train['card1_count_full'] = train['card1'].map(pd.concat([train['card1'], test['card1']], ignore_index=True).value_counts(dropna=False))

# addr1~2 (주소 count encoding)
train['addr1_count_full'] = train['addr1'].map(pd.concat([train['addr1'], test['addr1']], ignore_index=True).value_counts(dropna=False))

# Count encoded separately for train and test
['id_01', 'id_31', 'id_33', 'id_35']:

# two featrues label encoding
'id_02__id_20', 'id_02__D8', 
'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2', 
'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1'

# one featrue label encoding
'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 
'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29',
'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 
'DeviceType', 'DeviceInfo', 
'ProductCD', 
'card4', 'card6', 
'P_emaildomain','R_emaildomain', 
'P_emaildomain_bin','P_emaildomain_suffix','R_emaildomain_bin','R_emaildomain_suffix',  
'card1', 'card2', 'card3',  'card5', 
'addr1', 'addr2', 
'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9'

# feature aggregation

# 카드종류별(card4) 평균/표준편차 거래금액
TransactionAmt_to_mean_card1
TransactionAmt_to_mean_card4
TransactionAmt_to_std_card1
TransactionAmt_to_std_card4

# 카드종류별(card4) 평균/표준편차 id_02
id_02_to_mean_card1
id_02_to_mean_card4
id_02_to_std_card1
id_02_to_std_card4

# 카드종류별(card4) 평균/표준편차 거래금액차이
D15_to_mean_card1
D15_to_mean_card4
D15_to_std_card1
D15_to_std_card4

# 주소별 평균/표준편차 거래금액차이
D15_to_mean_addr1
D15_to_std_addr1

# drop features
1) many_null_cols
2) big_top_value_cols
3) one_value_cols

# missing value
repalce missing values with {num : mean, cat : -999}






