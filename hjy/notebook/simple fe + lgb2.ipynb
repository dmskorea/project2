{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# library \nimport numpy as np \nimport pandas as pd \nimport itertools\nfrom scipy import interp\nimport os\nimport time\nimport datetime\nimport gc\nimport json\nfrom numba import jit\nfrom itertools import product\nfrom tqdm import tqdm_notebook\n\n# Suppr warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\n\n# Data processing, metrics and modeling\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn import linear_model\n\n# ML\nimport lightgbm as lgb\n\n# options\npd.set_option('display.max_columns', 500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### read dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read\ntrain_transaction = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv')\ntest_transaction = pd.read_csv('../input/ieee-fraud-detection/test_transaction.csv')\ntrain_identity = pd.read_csv('../input/ieee-fraud-detection/train_identity.csv')\ntest_identity = pd.read_csv('../input/ieee-fraud-detection/test_identity.csv')\nsub = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv')\n\n# merge \ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')\n\n# reduce_mem_usage\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)\n\n# base_columns\nbase_columns = list(train) + list(train_identity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_transaction, train_identity, test_transaction, test_identity\n\nprint(\"Train shape : \"+str(train.shape))\nprint(\"Test shape  : \"+str(test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sampling \n\n# train = train.sample(5000)\n# test = test.sample(5000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FE : missing value"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['nulls1'] = train.isna().sum(axis=1)\ntest['nulls1'] = test.isna().sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FE : time of day"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\n\nSTART_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n\ntrain['dow'] = train['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x))).dt.dayofweek\ntrain['hour'] = train['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x))).dt.hour\ntrain['day'] = train['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x))).dt.day\ntest['dow'] = test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x))).dt.dayofweek\ntest['hour'] = test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x))).dt.hour\ntest['day'] = test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x))).dt.day","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### FE : id_31"},{"metadata":{"trusted":true},"cell_type":"code","source":"def setbrowser(df):\n    df.loc[df[\"id_31\"]==\"samsung browser 7.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"opera 53.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"mobile safari 10.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"google search application 49.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"firefox 60.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"edge 17.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 69.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 67.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for ios\",'lastest_browser']=1\n    return df\n\ntrain[\"lastest_browser\"] = np.zeros(train.shape[0])\ntest[\"lastest_browser\"] = np.zeros(test.shape[0])\ntrain = setbrowser(train)\ntest = setbrowser(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FE : email"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fe_email(df):   \n    \n    emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n    us_emails = ['gmail', 'net', 'edu']\n    \n    df['P_email'] = (df['P_emaildomain']=='xmail.com')\n    df['R_email'] = (df['R_emaildomain']=='xmail.com')\n    \n    df['P_isproton'] = (df['P_emaildomain']=='protonmail.com')\n    df['R_isproton'] = (df['R_emaildomain']=='protonmail.com')\n\n    df['email_check'] = np.where(df['P_emaildomain']==df['R_emaildomain'],1,0)\n    df['email_check_nan_all'] = np.where((df['P_emaildomain'].isna())&(df['R_emaildomain'].isna()),1,0)\n    df['email_check_nan_any'] = np.where((df['P_emaildomain'].isna())|(df['R_emaildomain'].isna()),1,0)    \n    df['email_match_not_nan'] = np.where( (df['P_emaildomain']==df['R_emaildomain']) & (np.invert(df['P_emaildomain'].isna())) ,1,0)\n    \n    df['P_emaildomain_bin'] = df['P_emaildomain'].map(emails)    \n    df['P_emaildomain_suffix'] = df['P_emaildomain'].map(lambda x: str(x).split('.')[-1])    \n    df['P_emaildomain_suffix'] = df['P_emaildomain_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    df['P_emaildomain_prefix'] = df['P_emaildomain'].map(lambda x: str(x).split('.')[0])   \n\n    df['R_emaildomain_bin'] = df['R_emaildomain'].map(emails)    \n    df['R_emaildomain_suffix'] = df['R_emaildomain'].map(lambda x: str(x).split('.')[-1])    \n    df['R_emaildomain_suffix'] = df['R_emaildomain_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    df['R_emaildomain_prefix'] = df['R_emaildomain'].map(lambda x: str(x).split('.')[0])   \n    \n    return df\n\ntrain = fe_email(train)\ntest = fe_email(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FE : card "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reset values for \"noise\" card1\nvalid_card = train['card1'].value_counts()\nvalid_card = valid_card[valid_card>10]\nvalid_card = list(valid_card.index)\n    \ntrain['card1'] = np.where(train['card1'].isin(valid_card), train['card1'], np.nan)\ntest['card1']  = np.where(test['card1'].isin(valid_card), test['card1'], np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# card3/5 low freq values \ntrain.loc[train.card3.isin(train.card3.value_counts()[train.card3.value_counts() < 200].index), 'card3'] = \"Others\"\ntest.loc[test.card3.isin(test.card3.value_counts()[test.card3.value_counts() < 200].index), 'card3'] = \"Others\"\n\ntrain.loc[train.card5.isin(train.card5.value_counts()[train.card5.value_counts() < 300].index), 'card5'] = \"Others\"\ntest.loc[test.card5.isin(test.card5.value_counts()[test.card5.value_counts() < 300].index), 'card5'] = \"Others\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def response_rate_by_group(df,x,y):\n    tmp = pd.crosstab(df[x],df[y])\n    tmp['Sum'] = tmp.apply(np.sum,axis=1)\n    tmp['Response_rate'] = tmp.loc[:,1]/tmp['Sum']\n    tmp = tmp.sort_values(['Response_rate'],ascending=False)\n    print(\"It would be interesting to see if the amount percentual is higher or lower than 3.5% of total!\")\n    return(tmp)\n\n# print(response_rate_by_group(df=train, x=\"card3\", y=\"isFraud\"))\n# print(response_rate_by_group(df=train, x=\"card5\", y=\"isFraud\"))\n\ntrain['card3_high_rate_fraud'] = np.where(train['card3'].isin([185,119,144]),1,0)\ntest['card3_high_rate_fraud'] = np.where(test['card3'].isin([185,119,144]),1,0)\n\ntrain['card5_high_rate_fraud'] = np.where(train['card5'].isin([137,147,141,223,138]),1,0)\ntest['card5_high_rate_fraud'] = np.where(test['card5'].isin([137,147,141,223,138]),1,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's add some kind of client uID based on cardID ad addr columns\ntrain['uid'] = train['card1'].astype(str)+'_'+train['card2'].astype(str)+'_'+train['card3'].astype(str)+'_'+train['card4'].astype(str)\ntest['uid'] = test['card1'].astype(str)+'_'+test['card2'].astype(str)+'_'+test['card3'].astype(str)+'_'+test['card4'].astype(str)\n\ntrain['uid2'] = train['uid'].astype(str)+'_'+train['addr1'].astype(str)+'_'+train['addr2'].astype(str)\ntest['uid2'] = test['uid'].astype(str)+'_'+test['addr1'].astype(str)+'_'+test['addr2'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding - count encoding for both train and test\nfor feature in ['card1', 'card2', 'card3', 'card4', 'card5', 'card6','uid','uid2']:\n    train[feature + '_count_full'] = train[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n    test[feature + '_count_full'] = test[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Anomaly Search in geo information\n# Let's look on bank addres and client addres matching\n# card3/card5 bank country and name?\n# Addr2 -> Clients geo position (country)\n# Most common entries -> normal transactions\n# Less common etries -> some anonaly\ntrain['bank_type'] = train['card3'].astype(str)+'_'+train['card5'].astype(str)\ntest['bank_type']  = test['card3'].astype(str)+'_'+test['card5'].astype(str)\n\ntrain['address_match'] = train['bank_type'].astype(str)+'_'+train['addr2'].astype(str)\ntest['address_match']  = test['bank_type'].astype(str)+'_'+test['addr2'].astype(str)\n\nfor col in ['address_match','bank_type']:\n    tmp = pd.concat([train[[col]], test[[col]]])\n    tmp[col] = np.where(tmp[col].str.contains('nan'), np.nan, tmp[col])\n    tmp = tmp.dropna()\n    fq_encode = tmp[col].value_counts().to_dict()   \n    train[col] = train[col].map(fq_encode)\n    test[col]  = test[col].map(fq_encode)\n\ntrain['address_match'] = train['address_match']/train['bank_type'] \ntest['address_match']  = test['address_match']/test['bank_type']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FE : D9 (hour)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['local_hour'] = train['D9']*24\ntest['local_hour']  = test['D9']*24\n\ntrain['local_hour'] = train['local_hour'] - (train['TransactionDT']/(60*60))%24\ntest['local_hour']  = test['local_hour'] - (test['TransactionDT']/(60*60))%24\n\ntrain['local_hour_dist'] = train['local_hour']/train['dist2']\ntest['local_hour_dist']  = test['local_hour']/test['dist2']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FE : M1 ~ M9 (binary encoding, except M4)"},{"metadata":{"trusted":true},"cell_type":"code","source":"i_cols = ['M1','M2','M3','M5','M6','M7','M8','M9']\n\ntrain['M_sum'] = train[i_cols].sum(axis=1).astype(np.int8)\ntest['M_sum']  = test[i_cols].sum(axis=1).astype(np.int8)\n\ntrain['M_na'] = train[i_cols].isna().sum(axis=1).astype(np.int8)\ntest['M_na']  = test[i_cols].isna().sum(axis=1).astype(np.int8)\n\ntrain['M_type'] = ''\ntest['M_type']  = ''\n\nfor col in i_cols:\n    train['M_type'] = '_'+train[col].astype(str)\n    test['M_type'] = '_'+test[col].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FE : addr "},{"metadata":{"trusted":true},"cell_type":"code","source":"train['addr1_count_full'] = train['addr1'].map(pd.concat([train['addr1'], test['addr1']], ignore_index=True).value_counts(dropna=False))\ntest['addr1_count_full'] = test['addr1'].map(pd.concat([train['addr1'], test['addr1']], ignore_index=True).value_counts(dropna=False))\n\ntrain['addr2_count_full'] = train['addr2'].map(pd.concat([train['addr2'], test['addr2']], ignore_index=True).value_counts(dropna=False))\ntest['addr2_count_full'] = test['addr2'].map(pd.concat([train['addr2'], test['addr2']], ignore_index=True).value_counts(dropna=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FE : device "},{"metadata":{"trusted":true},"cell_type":"code","source":"def id_split(df):\n    df['device_name'] = df['DeviceInfo'].str.split('/', expand=True)[0]\n    df['device_version'] = df['DeviceInfo'].str.split('/', expand=True)[1]\n\n    df['OS_id_30'] = df['id_30'].str.split(' ', expand=True)[0]\n    df['version_id_30'] = df['id_30'].str.split(' ', expand=True)[1]\n\n    df['browser_id_31'] = df['id_31'].str.split(' ', expand=True)[0]\n    df['version_id_31'] = df['id_31'].str.split(' ', expand=True)[1]\n\n    df['screen_width'] = df['id_33'].str.split('x', expand=True)[0]\n    df['screen_height'] = df['id_33'].str.split('x', expand=True)[1]\n\n    #df['id_34'] = df['id_34'].str.split(':', expand=True)[1]\n    #df['id_23'] = df['id_23'].str.split(':', expand=True)[1]\n\n    df.loc[df['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n    df.loc[df['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n    df.loc[df['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n    df.loc[df['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n    df.loc[df['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n    df.loc[df['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n    df.loc[df['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n    df.loc[df['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n\n    df.loc[df.device_name.isin(df.device_name.value_counts()[df.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n    df['had_id'] = 1\n    gc.collect()\n    \n    return df\n\ntrain = id_split(train)\ntest = id_split(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FE : TransactionAmt"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['TransactionAmt_log'] = np.log(train['TransactionAmt'])\ntest['TransactionAmt_log'] = np.log(test['TransactionAmt'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\ntest['TransactionAmt_decimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['TransactionAmt_check'] = np.where(train['TransactionAmt'].isin(test['TransactionAmt']), 1, 0)\ntest['TransactionAmt_check']  = np.where(test['TransactionAmt'].isin(train['TransactionAmt']), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For our model current TransactionAmt is a noise (even when features importances are telling contrariwise)\n# There are many unique values and model doesn't generalize well, Lets do some aggregations\ni_cols = ['card1','card2','card3','card5','uid','uid2']\n\nfor col in i_cols:\n    for agg_type in ['mean', 'std' ,'sum']:\n        new_col_name = col+'_TransactionAmt_'+agg_type\n        tmp = pd.concat([train[[col, 'TransactionAmt']], test[[col,'TransactionAmt']]])\n        tmp = tmp.groupby([col])['TransactionAmt'].agg([agg_type]).reset_index().rename(columns={agg_type: new_col_name})\n        \n        tmp.index = list(tmp[col])\n        tmp = tmp[new_col_name].to_dict()   \n    \n        train[new_col_name] = train[col].map(tmp)\n        test[new_col_name]  = test[col].map(tmp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FE : two featrues label encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in ['id_02__id_20', 'id_02__D8', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2', 'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1']:\n\n    f1, f2 = feature.split('__')\n    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n\n    le = preprocessing.LabelEncoder()\n    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n    train[feature] = le.transform(list(train[feature].astype(str).values))\n    test[feature] = le.transform(list(test[feature].astype(str).values))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FE : count encoded separately for train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in ['id_01', 'id_31', 'id_33', 'id_35']:\n    train[feature + '_count_dist'] = train[feature].map(train[feature].value_counts(dropna=False))\n    test[feature + '_count_dist'] = test[feature].map(test[feature].value_counts(dropna=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FE : Freq encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"i_cols = ['card1','card2','card3','card5',\n          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n          'D1','D2','D3','D4','D5','D6','D7','D8','D9',\n          'addr1','addr2',\n          'dist1','dist2',\n          'P_emaildomain', 'R_emaildomain',\n          'id_01','id_02','id_03','id_04','id_05','id_06','id_07','id_08','id_09','id_10',\n          'id_11','id_13','id_14','id_17','id_18','id_19','id_20','id_21','id_22','id_24',\n          'id_25','id_26','id_30','id_31','id_32','id_33',\n          'DeviceInfo'\n         ]\n\nfor col in i_cols:\n    tmp = pd.concat([train[[col]], test[[col]]])\n    fq_encode = tmp[col].value_counts().to_dict()   \n    train[col+'_fq_enc'] = train[col].map(fq_encode)\n    test[col+'_fq_enc']  = test[col].map(fq_encode)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FE : ProductCD and M4 Target mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['ProductCD','M4']:\n    temp_dict = train.groupby([col])['isFraud'].agg(['mean']).reset_index().rename(columns={'mean': col+'_target_mean'})\n    temp_dict.index = temp_dict[col].values\n    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n\n    train[col+'_target_mean'] = train[col].map(temp_dict)\n    test[col+'_target_mean']  = test[col].map(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FE : feature aggregation "},{"metadata":{"trusted":true},"cell_type":"code","source":"# For our model current TransactionAmt is a noise (even when features importances are telling contrariwise)\n# There are many unique values and model doesn't generalize well, Lets do some aggregations\n\ni_cols = ['card1','card2','card3','card5','uid','uid2']\nfor col in i_cols:\n    for agg_type in ['mean', 'std' ,'sum']:\n        new_col_name = col+'_'+'TransactionAmt'+'_'+agg_type\n        tmp = pd.concat([train[[col, 'TransactionAmt']], test[[col,'TransactionAmt']]])\n        tmp = tmp.groupby([col])['TransactionAmt'].agg([agg_type]).reset_index().rename(columns={agg_type: new_col_name})\n        \n        tmp.index = list(tmp[col])\n        tmp = tmp[new_col_name].to_dict()   \n    \n        train[new_col_name] = train[col].map(tmp)\n        test[new_col_name]  = test[col].map(tmp)\n\ni_cols = ['card1','card2','card3','card5','uid','uid2']\nfor col in i_cols:\n    for agg_type in ['mean', 'std' ,'sum']:\n        new_col_name = col+'_'+'D15'+'_'+agg_type\n        tmp = pd.concat([train[[col, 'D15']], test[[col,'D15']]])\n        tmp = tmp.groupby([col])['D15'].agg([agg_type]).reset_index().rename(columns={agg_type: new_col_name})\n        \n        tmp.index = list(tmp[col])\n        tmp = tmp[new_col_name].to_dict()   \n    \n        train[new_col_name] = train[col].map(tmp)\n        test[new_col_name]  = test[col].map(tmp)\n        \ni_cols = ['addr1','addr2']\nfor col in i_cols:\n    for agg_type in ['mean', 'std' ,'sum']:\n        new_col_name = col+'_'+'D15'+'_'+agg_type\n        tmp = pd.concat([train[[col, 'D15']], test[[col,'D15']]])\n        tmp = tmp.groupby([col])['D15'].agg([agg_type]).reset_index().rename(columns={agg_type: new_col_name})\n        \n        tmp.index = list(tmp[col])\n        tmp = tmp[new_col_name].to_dict()   \n    \n        train[new_col_name] = train[col].map(tmp)\n        test[new_col_name]  = test[col].map(tmp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FE : repalce missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill in mean for floats\n# for c in train.columns:\n#     if train[c].dtype=='float16' or  train[c].dtype=='float32' or  train[c].dtype=='float64':\n#         train[c].fillna(train[c].mean())\n#         train[c].fillna(train[c].mean())\n\n# fill in -999 for categoricals\n# train = train.fillna(-999)\n# test = test.fillna(-999)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FE : character feature encoding "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encode Str columns\nfor col in list(train):\n    if train[col].dtype=='O':\n        print(col)\n        train[col] = train[col].fillna('unseen_before_label')\n        test[col]  = test[col].fillna('unseen_before_label')\n        \n        train[col] = train[col].astype(str)\n        test[col] = test[col].astype(str)\n        \n        le = LabelEncoder()\n        le.fit(list(train[col])+list(test[col]))\n        train[col] = le.transform(train[col])\n        test[col]  = le.transform(test[col])\n        \n        train[col] = train[col].astype('category')\n        test[col] = test[col].astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FE : drop features "},{"metadata":{"trusted":true},"cell_type":"code","source":"many_null_cols = [col for col in train.columns if train[col].isnull().sum() / train.shape[0] > 0.9]\nmany_null_cols_test = [col for col in test.columns if test[col].isnull().sum() / test.shape[0] > 0.9]\nbig_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\nbig_top_value_cols_test = [col for col in test.columns if test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\none_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\none_value_cols_test = [col for col in test.columns if test[col].nunique() <= 1]\n\nprint(\">> many_null_cols :\",many_null_cols)\nprint(\">> big_top_value_cols :\",big_top_value_cols)\nprint(\">> one_value_cols :\",one_value_cols)\n\ncols_to_drop = list(set(many_null_cols + many_null_cols_test + big_top_value_cols + big_top_value_cols_test + one_value_cols + one_value_cols_test))\ncols_to_drop.remove('isFraud')\n\ntrain = train.drop(cols_to_drop, axis=1)\ntest = test.drop(cols_to_drop, axis=1)\n\nprint(\">> num of cols_to_drop :\",len(cols_to_drop))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### model"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\">> num of columns :\",len(train.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['TransactionID','uid','uid2','bank_type'],axis=1)\ntest = test.drop(['TransactionID','uid','uid2','bank_type'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT'], axis=1)\ny = train.sort_values('TransactionDT')['isFraud']\nX_test = test.drop(['TransactionDT'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'objective':'binary',\n    'boosting_type':'gbdt',\n    'metric':'auc',\n    'n_jobs':-1,\n    'learning_rate':0.01,\n    'num_leaves': 2**8,\n    'max_depth':-1,\n    'tree_learner':'serial',\n    'colsample_bytree': 0.7,\n    'subsample_freq':1,\n    'subsample':1,\n    'n_estimators':800,\n    'max_bin':255,\n    'verbose':-1,\n    'seed': 1234,\n    'early_stopping_rounds':100, \n} ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nNFOLDS = 5\nfolds = KFold(n_splits=NFOLDS)\n\ncolumns = X.columns\nsplits = folds.split(X, y)\ny_preds = np.zeros(X_test.shape[0])\ny_oof = np.zeros(X.shape[0])\nscore = 0\n\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = columns\n  \nfor fold_n, (train_index, valid_index) in enumerate(splits):\n    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    dtrain = lgb.Dataset(X_train, label=y_train)\n    dvalid = lgb.Dataset(X_valid, label=y_valid)\n\n    clf = lgb.train(params, dtrain, 10000, valid_sets = [dtrain, dvalid], verbose_eval=200, early_stopping_rounds=500)\n    \n    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n    \n    y_pred_valid = clf.predict(X_valid)\n    y_oof[valid_index] = y_pred_valid\n    print(f\"Fold {fold_n + 1} | AUC: {roc_auc_score(y_valid, y_pred_valid)}\")\n    \n    score += roc_auc_score(y_valid, y_pred_valid) / NFOLDS\n    y_preds += clf.predict(X_test) / NFOLDS\n    \n    del X_train, X_valid, y_train, y_valid\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"\\nMean AUC = {score}\")\nprint(f\"Out of folds AUC = {roc_auc_score(y, y_oof)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['isFraud'] = y_preds\nsub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(folds.n_splits)]].mean(axis=1)\nfeature_importances.to_csv('feature_importances.csv')\nplt.figure(figsize=(16, 16))\nsns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\nplt.title('50 TOP feature importance over {} folds average'.format(folds.n_splits));","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}